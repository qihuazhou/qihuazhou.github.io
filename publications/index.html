<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Qihua Zhou (周祺華) </title> <meta name="author" content="Qihua Zhou (周祺華) "> <meta name="description" content=""> <meta name="keywords" content="Edge AI Systems, Neural-enhanced Video Streaming, Tiny Machine learning, On-device Learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic_square.jpg?847ffc68933434fa2abed973276c0886"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qihuazhou.github.io/publications/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Qihua Zhou (周祺華)</span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/bio/">Bio </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/services/">Services </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE TPAMI’24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pass_tpami24-480.webp 480w,/assets/img/publication_preview/pass_tpami24-800.webp 800w,/assets/img/publication_preview/pass_tpami24-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/pass_tpami24" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pass_tpami24" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:journals/pami/ZhouGPLGXZ24" class="col-sm-8"> <div class="title">PASS: Patch Automatic Skip Scheme for Efficient On-Device Video Perception</div> <div class="author"> <em>Qihua Zhou</em>, Song Guo, Jun Pan, Jiacheng Liang, Jingcai Guo, Zhenda Xu, and Jingren Zhou </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI, CCF-A, IF=23.6)</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10381763" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Real-time video perception tasks are often challenging on resource-constrained edge devices due to the issues of accuracy drop and hardware overhead, where saving computations is the key to performance improvement. Existing methods either rely on domain-specific neural chips or priorly searched models, which require specialized optimization according to different task properties. These limitations motivate us to design a general and task-independent methodology, called Patch Automatic Skip Scheme (PASS), which supports diverse video perception settings by decoupling acceleration and tasks. The gist is to capture inter-frame correlations and skip redundant computations at patch level, where the patch is a non-overlapping square block in visual. PASS equips each convolution layer with a learnable gate to selectively determine which patches could be safely skipped without degrading model accuracy. Specifically, we are the first to construct a self-supervisory procedure for gate optimization, which learns to extract contrastive representations from frame sequences. The pre-trained gates can serve as plug-and-play modules to implement patch-skippable neural backbones, and automatically generate proper skip strategy to accelerate different video-based downstream tasks, e.g., outperforming state-of-the-art MobileHumanPose in 3D pose estimation and FairMOT in multiple object tracking, by up to 9.43× and 12.19× speedups, respectively, on NVIDIA Jetson Nano devices.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/pami/ZhouGPLGXZ24</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Guo, Song and Pan, Jun and Liang, Jiacheng and Guo, Jingcai and Xu, Zhenda and Zhou, Jingren}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{PASS:} Patch Automatic Skip Scheme for Efficient On-Device Video
                    Perception}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{IEEE} Transactions on Pattern Analysis and Machine Intelligence (TPAMI, CCF-A, IF=23.6)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{46}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3938--3954}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/TPAMI.2024.3350380}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TPAMI.2024.3350380}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Sat, 04 May 2024 10:55:20 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/journals/pami/ZhouGPLGXZ24.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE TMC’24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tc_tmc24-480.webp 480w,/assets/img/publication_preview/tc_tmc24-800.webp 800w,/assets/img/publication_preview/tc_tmc24-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/tc_tmc24" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tc_tmc24" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10382540" class="col-sm-8"> <div class="title">Chiron: A Robustness-Aware Incentive Scheme for Edge Learning Via Hierarchical Reinforcement Learning</div> <div class="author"> Yi Liu, Song Guo, Yufeng Zhan, Leijie Wu, Zicong Hong, and <em>Qihua Zhou</em> </div> <div class="periodical"> <em>IEEE Transactions on Mobile Computing (TMC, CCF-A, IF=7.9)</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10382540" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Over the past few years, edge learning has achieved significant success in mobile edge networks. Few works have designed incentive mechanism that motivates edge nodes to participate in edge learning. However, most existing works only consider myopic optimization and assume that all edge nodes are honest, which lacks long-term sustainability and the final performance assurance. In this paper, we propose Chiron, an incentive-driven Byzantine-resistant long-term mechanism based on hierarchical reinforcement learning (HRL). First, our optimization goal includes both learning-algorithm performance criteria (i.e., global accuracy) and systematical criteria (i.e., resource consumption), which aim to improve the edge learning performance under a given resource budget. Second, we propose a three-layer HRL architecture to handle long-term optimization, short-term optimization, and byzantine resistance, respectively. Finally, we conduct experiments on various edge learning tasks to demonstrate the superiority of the proposed approach. Specifically, our system can successfully exclude malicious nodes and lazy nodes out of the edge learning participation and achieves 14.96% higher accuracy and 12.66% higher total utility than the state-of-the-art methods under the same budget limit.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10382540</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Yi and Guo, Song and Zhan, Yufeng and Wu, Leijie and Hong, Zicong and Zhou, Qihua}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Mobile Computing (TMC, CCF-A, IF=7.9)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Chiron: A Robustness-Aware Incentive Scheme for Edge Learning Via Hierarchical Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-17}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Training;Servers;Optimization;Deep learning;Computational modeling;Reinforcement learning;Data models;Deep reinforcement learning;edge learning;incentive mechanism;mobile edge computing}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TMC.2024.3350654}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI’24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/nevs_aaai24-480.webp 480w,/assets/img/publication_preview/nevs_aaai24-800.webp 800w,/assets/img/publication_preview/nevs_aaai24-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/nevs_aaai24" class="preview z-depth-1 rounded" width="100%" height="auto" alt="nevs_aaai24" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:conf/aaai/ZhouGGLZWX24" class="col-sm-8"> <div class="title">On the Robustness of Neural-Enhanced Video Streaming against Adversarial Attacks</div> <div class="author"> <em>Qihua Zhou</em>, Jingcai Guo, Song Guo, Ruibin Li, Jie Zhang, Bingjie Wang, and Zhenda Xu </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI, CCF-A)</em> , Vancouver, Canada, Feb 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29657" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The explosive growth of video traffic on today’s Internet promotes the rise of Neural-enhanced Video Streaming (NeVS), which effectively improves the rate-distortion trade-off by employing a cheap neural super-resolution model for quality enhancement on the receiver side. Missing by existing work, we reveal that the NeVS pipeline may suffer from a practical threat, where the crucial codec component (i.e., encoder for compression and decoder for restoration) can trigger adversarial attacks in a man-in-the-middle manner to significantly destroy video recovery performance and finally incurs the malfunction of downstream video perception tasks. In this paper, we are the first attempt to inspect the vulnerability of NeVS and discover a novel adversarial attack, called codec hijacking, where the injected invisible perturbation conspires with the malicious encoding matrix by reorganizing the spatial-temporal bit allocation within the bitstream size budget. Such a zero-day vulnerability makes our attack hard to defend because there is no visual distortion on the recovered videos until the attack happens. More seriously, this attack can be extended to diverse enhancement models, thus exposing a wide range of video perception tasks under threat. Evaluation based on state-of-the-art video codec benchmark illustrates that our attack significantly degrades the recovery performance of NeVS over previous attack methods. The damaged video quality finally leads to obvious malfunction of downstream tasks with over 75% success rate. We hope to arouse public attention on codec hijacking and its defence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/aaai/ZhouGGLZWX24</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Guo, Jingcai and Guo, Song and Li, Ruibin and Zhang, Jie and Wang, Bingjie and Xu, Zhenda}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Wooldridge, Michael J. and Dy, Jennifer G. and Natarajan, Sriraam}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Robustness of Neural-Enhanced Video Streaming against Adversarial
                    Attacks}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the {AAAI} Conference on Artificial Intelligence (AAAI, CCF-A)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{17123--17131}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{AAAI} Press}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{ Vancouver, Canada}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1609/aaai.v38i15.29657}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/AAAI.V38I15.29657}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Tue, 02 Apr 2024 16:32:09 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/aaai/ZhouGGLZWX24.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE TMC’23</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tree_tmc23-480.webp 480w,/assets/img/publication_preview/tree_tmc23-800.webp 800w,/assets/img/publication_preview/tree_tmc23-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/tree_tmc23" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tree_tmc23" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:journals/tmc/GuoGWXZZCZ24" class="col-sm-8"> <div class="title">Tree Learning: Towards Promoting Coordination in Scalable Multi-Client Training Acceleration</div> <div class="author"> Tao Guo, Song Guo, Feijie Wu, Wenchao Xu, Jiewei Zhang, <em>Qihua Zhou</em>, Quan Chen, and Weihua Zhuang </div> <div class="periodical"> <em>IEEE Transactions on Mobile Computing (TMC, CCF-A, IF=7.9)</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/10076834" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Iteration based collaborative learning (CL) paradigms, such as federated learning (FL) and split learning (SL), faces challenges in training neural models over the rapidly growing yet resource-constrained edge devices. Such devices have difficulty in accommodating a full-size large model for FL or affording an excessive waiting time for the mandatory synchronization step in SL. To deal with such challenge, we propose a novel CL framework which adopts an tree-aggregation structure with an adaptive partition and ensemble strategy to achieve optimal synchronization and fast convergence at scale. To find the optimal split point for heterogeneous clients, we also design a novel partitioning algorithm by minimizing the idleness during communication and achieving the optimal synchronization between clients. In addition, a parallelism paradigm is proposed to unleash the potential of optimum synchronization between the clients and server to boost the distributed training process without losing model accuracy for edge devices. Furthermore, we theoretically prove that our framework can achieve better convergence rate than state-of-the-art CL paradigms. We conduct extensive experiments and show that our framework is 4.6× in training speed as compared with the traditional methods, without compromising training accuracy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/tmc/GuoGWXZZCZ24</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo, Tao and Guo, Song and Wu, Feijie and Xu, Wenchao and Zhang, Jiewei and Zhou, Qihua and Chen, Quan and Zhuang, Weihua}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tree Learning: Towards Promoting Coordination in Scalable Multi-Client
                    Training Acceleration}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Mobile Computing (TMC, CCF-A, IF=7.9)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2382--2394}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/TMC.2023.3259007}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TMC.2023.3259007}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Thu, 29 Feb 2024 20:54:12 +0100}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/journals/tmc/GuoGWXZZCZ24.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI’23</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pass_aaai23-480.webp 480w,/assets/img/publication_preview/pass_aaai23-800.webp 800w,/assets/img/publication_preview/pass_aaai23-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/pass_aaai23" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pass_aaai23" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:conf/aaai/Zhou0PLXZ23" class="col-sm-8"> <div class="title">PASS: Patch Automatic Skip Scheme for Efficient Real-Time Video Perception on Edge Devices</div> <div class="author"> <em>Qihua Zhou</em>, Song Guo, Jun Pan, Jiacheng Liang, Zhenda Xu, and Jingren Zhou </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI, CCF-A)</em> , Washington, DC, USA, Feb 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25491" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Real-time video perception tasks are often challenging over the resource-constrained edge devices due to the concerns of accuracy drop and hardware overhead, where saving computations is the key to performance improvement. Existing methods either rely on domain-specific neural chips or priorly searched models, which require specialized optimization according to different task properties. In this work, we propose a general and task-independent Patch Automatic Skip Scheme (PASS), a novel end-to-end learning pipeline to support diverse video perception settings by decoupling acceleration and tasks. The gist is to capture the temporal similarity across video frames and skip the redundant computations at patch level, where patch is a non-overlapping square block in visual. PASS equips each convolution layer with a learnable gate to selectively determine which patches could be safely skipped without degrading model accuracy. As to each layer, a desired gate needs to make flexible skip decisions based on intermediate features without any annotations, which cannot be achieved by conventional supervised learning paradigm. To address this challenge, we are the first to construct a tough self-supervisory procedure for optimizing these gates, which learns to extract contrastive representation, i.e., distinguishing similarity and difference, from frame sequence. These high-capacity gates can serve as a plug-and-play module for convolutional neural network (CNN) backbones to implement patch-skippable architectures, and automatically generate proper skip strategy to accelerate different video-based downstream tasks, e.g., outperforming the state-of-the-art MobileHumanPose (MHP) in 3D pose estimation and FairMOT in multiple object tracking, by up to 9.43 times and 12.19 times speedups, respectively. By directly processing the raw data of frames, PASS can generalize to real-time video streams on commodity edge devices, e.g., NVIDIA Jetson Nano and mobile phones, and achieves efficient performance in realistic deployment.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/aaai/Zhou0PLXZ23</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Guo, Song and Pan, Jun and Liang, Jiacheng and Xu, Zhenda and Zhou, Jingren}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Williams, Brian and Chen, Yiling and Neville, Jennifer}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{PASS:} Patch Automatic Skip Scheme for Efficient Real-Time Video
                    Perception on Edge Devices}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the {AAAI} Conference on Artificial Intelligence (AAAI, CCF-A)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3787--3795}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{AAAI} Press}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Washington, DC, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1609/aaai.v37i3.25491}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/AAAI.V37I3.25491}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Tue, 07 May 2024 20:01:46 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/aaai/Zhou0PLXZ23.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI’23</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/zsl_aaai23-480.webp 480w,/assets/img/publication_preview/zsl_aaai23-800.webp 800w,/assets/img/publication_preview/zsl_aaai23-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/zsl_aaai23" class="preview z-depth-1 rounded" width="100%" height="auto" alt="zsl_aaai23" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:conf/aaai/GuoGZLLH23" class="col-sm-8"> <div class="title">Graph Knows Unknowns: Reformulate Zero-Shot Learning as Sample-Level Graph Recognition</div> <div class="author"> Jingcai Guo, Song Guo, <em>Qihua Zhou</em>, Ziming Liu, Xiaocheng Lu, and Fushuo Huo </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI, CCF-A)</em> , Washington, DC, USA, Feb 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25942" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Zero-shot learning (ZSL) is an extreme case of transfer learning that aims to recognize samples (e.g., images) of unseen classes relying on a train-set covering only seen classes and a set of auxiliary knowledge (e.g., semantic descriptors). Existing methods usually resort to constructing a visual-to-semantics mapping based on features extracted from each whole sample. However, since the visual and semantic spaces are inherently independent and may exist in different manifolds, these methods may easily suffer from the domain bias problem due to the knowledge transfer from seen to unseen classes. Unlike existing works, this paper investigates the fine-grained ZSL from a novel perspective of sample-level graph. Specifically, we decompose an input into several fine-grained elements and construct a graph structure per sample to measure and utilize element-granularity relations within each sample. Taking advantage of recently developed graph neural networks (GNNs), we formulate the ZSL problem to a graph-to-semantics mapping task, which can better exploit element-semantics correlation and local sub-structural information in samples. Experimental results on the widely used benchmark datasets demonstrate that the proposed method can mitigate the domain bias problem and achieve competitive performance against other representative methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/aaai/GuoGZLLH23</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo, Jingcai and Guo, Song and Zhou, Qihua and Liu, Ziming and Lu, Xiaocheng and Huo, Fushuo}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Williams, Brian and Chen, Yiling and Neville, Jennifer}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Graph Knows Unknowns: Reformulate Zero-Shot Learning as Sample-Level
                    Graph Recognition}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the {AAAI} Conference on Artificial Intelligence (AAAI, CCF-A)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7775--7783}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{AAAI} Press}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Washington, DC, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1609/aaai.v37i6.25942}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/AAAI.V37I6.25942}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Mon, 04 Sep 2023 16:50:24 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/aaai/GuoGZLLH23.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS’22</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sgq_nips22-480.webp 480w,/assets/img/publication_preview/sgq_nips22-800.webp 800w,/assets/img/publication_preview/sgq_nips22-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/sgq_nips22" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sgq_nips22" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:conf/nips/Zhou0LZZGXLQ22" class="col-sm-8"> <div class="title">Hierarchical Channel-spatial Encoding for Communication-efficient Collaborative Learning</div> <div class="author"> <em>Qihua Zhou</em>, Song Guo, Yi Liu, Jie Zhang, Jiewei Zhang, Tao Guo, Zhenda Xu, Xun Liu, and Zhihao Qu </div> <div class="periodical"> <em>In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS, CCF-A)</em> , New Orleans, USA, Nov 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/2616697705f72f16a8eac9c295d37d94-Supplemental-Conference.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://nips.cc/media/neurips-2022/Slides/54328_aZV9Kj5.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </div> <div class="abstract hidden"> <p>It witnesses that the collaborative learning (CL) systems often face the performance bottleneck of limited bandwidth, where multiple low-end devices continuously generate data and transmit intermediate features to the cloud for incremental training. To this end, improving the communication efficiency by reducing traffic size is one of the most crucial issues for realistic deployment. Existing systems mostly compress features at pixel level and ignore the characteristics of feature structure, which could be further exploited for more efficient compression. In this paper, we take new insights into implementing scalable CL systems through a hierarchical compression on features, termed Stripe-wise Group Quantization (SGQ). Different from previous unstructured quantization methods, SGQ captures both channel and spatial similarity in pixels, and simultaneously encodes features in these two levels to gain a much higher compression ratio. In particular, we refactor feature structure based on inter-channel similarity and bound the gradient deviation caused by quantization, in forward and backward passes, respectively. Such a double-stage pipeline makes SGQ hold a sublinear convergence order as the vanilla SGD-based optimization. Extensive experiments show that SGQ achieves a higher traffic reduction ratio by up to 15.97 times and provides 9.22 times image processing speedup over the uniform quantized training, while preserving adequate model accuracy as FP32 does, even using 4-bit quantization. This verifies that SGQ can be applied to a wide spectrum of edge intelligence applications.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/nips/Zhou0LZZGXLQ22</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Guo, Song and Liu, Yi and Zhang, Jie and Zhang, Jiewei and Guo, Tao and Xu, Zhenda and Liu, Xun and Qu, Zhihao}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Koyejo, Sanmi and Mohamed, S. and Agarwal, A. and Belgrave, Danielle and Cho, K. and Oh, A.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hierarchical Channel-spatial Encoding for Communication-efficient
                    Collaborative Learning}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS, CCF-A)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{New Orleans, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://papers.nips.cc/paper\_files/paper/2022/hash/2616697705f72f16a8eac9c295d37d94-Abstract-Conference.html}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Mon, 08 Jan 2024 16:31:37 +0100}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/nips/Zhou0LZZGXLQ22.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE TC Spotlight</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/falcon_tc_spotlight21-480.webp 480w,/assets/img/publication_preview/falcon_tc_spotlight21-800.webp 800w,/assets/img/publication_preview/falcon_tc_spotlight21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/falcon_tc_spotlight21" class="preview z-depth-1 rounded" width="100%" height="auto" alt="falcon_tc_spotlight21" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:journals/computer/ZhouGLLGSW21" class="col-sm-8"> <div class="title">A Comprehensive Inspection of the Straggler Problem</div> <div class="author"> <em>Qihua Zhou</em>, Song Guo, Haodong Lu, Li Li, Minyi Guo, Yanfei Sun, and Kun Wang </div> <div class="periodical"> <em>The Spotlight of IEEE Transactions on Computers</em>, Sep 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9548015" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Parameter server is a popular distributed processing paradigm for operating distributed deep learning (DL) applications. As a growing number of DL models are trained via shared clusters, machines are in confrontation with the heterogeneous environment, which incurs the unexpected phenomenon with a slow task processing speed called straggler. Straggler addressing is a crucial issue in distributed DL applications, since stragglers significantly hamper system performance. While many techniques have been deployed to mitigate stragglers, they may not achieve their goals with the presence of heterogeneity, where systems consume much longer time until DL training convergence than in a homogeneous environment, as evidenced by our experimental study. With the methodology of straggler projection and abstraction of parallelism, a new synchronization mechanism called elastic parallelism synchronous parallel (EPSP) is proposed, which exploits the superiority of iteration acceleration in stale synchronous parallel and conquers the shortage of barrier wasting time in bulk synchronous parallel. More precisely, EPSP supports both enforced and slack synchronization by adjusting the parameter of staleness.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/computer/ZhouGLLGSW21</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Guo, Song and Lu, Haodong and Li, Li and Guo, Minyi and Sun, Yanfei and Wang, Kun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Comprehensive Inspection of the Straggler Problem}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Spotlight of IEEE Transactions on Computers}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{54}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4--5}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/MC.2021.3099211}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/MC.2021.3099211}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Mon, 15 May 2023 18:00:34 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/journals/computer/ZhouGLLGSW21.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE IoTJ’21</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/survey_iotj21-480.webp 480w,/assets/img/publication_preview/survey_iotj21-800.webp 800w,/assets/img/publication_preview/survey_iotj21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/survey_iotj21" class="preview z-depth-1 rounded" width="100%" height="auto" alt="survey_iotj21" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:journals/iotj/ZhouQGLGXA21" class="col-sm-8"> <div class="title">On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective</div> <div class="author"> <em>Qihua Zhou</em>, Zhihao Qu, Song Guo, Boyuan Luo, Jingcai Guo, Zhenda Xu, and Rajendra Akerkar </div> <div class="periodical"> <em>IEEE Internet of Things Journal (IoTJ, JCR-Q1, IF=10.6)</em>, Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9366901" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Modern machine learning (ML) applications are often deployed in the cloud environment to exploit the computational power of clusters. However, this in-cloud computing scheme cannot satisfy the demands of emerging edge intelligence scenarios, including providing personalized models, protecting user privacy, adapting to real-time tasks, and saving resource cost. In order to conquer the limitations of conventional in-cloud computing, there comes the rise of on-device learning, which makes the end-to-end ML procedure totally on user devices, without unnecessary involvement of the cloud. In spite of the promising advantages of on-device learning, implementing a high-performance on-device learning system still faces with many severe challenges, such as insufficient user training data, backward propagation (BP) blocking, and limited peak processing speed. Observing the substantial improvement space in the implementation and acceleration of on-device learning systems, we intend to present a comprehensive analysis of the latest research progress and point out potential optimization directions from the system perspective. This survey presents a software and hardware synergy of on-device learning techniques, covering the scope of model-level neural network design, algorithm-level training optimization, and hardware-level instruction acceleration. We hope this survey could bring fruitful discussions and inspire the researchers to further promote the field of edge intelligence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/iotj/ZhouQGLGXA21</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Qu, Zhihao and Guo, Song and Luo, Boyuan and Guo, Jingcai and Xu, Zhenda and Akerkar, Rajendra}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On-Device Learning Systems for Edge Intelligence: {A} Software and
                    Hardware Synergy Perspective}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Internet of Things Journal (IoTJ, JCR-Q1, IF=10.6)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{11916--11934}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/JIOT.2021.3063147}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/JIOT.2021.3063147}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Thu, 16 Sep 2021 18:02:07 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/journals/iotj/ZhouQGLGXA21.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">USENIX ATC’21</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/octo_atc21-480.webp 480w,/assets/img/publication_preview/octo_atc21-800.webp 800w,/assets/img/publication_preview/octo_atc21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/octo_atc21" class="preview z-depth-1 rounded" width="100%" height="auto" alt="octo_atc21" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:conf/usenix/Zhou0QGXZGLZ21" class="col-sm-8"> <div class="title">Octo: INT8 Training with Loss-aware Compensation and Backward Quantization for Tiny On-device Learning</div> <div class="author"> <em>Qihua Zhou</em>, Song Guo, Zhihao Qu, Jingcai Guo, Zhenda Xu, Jiewei Zhang, Tao Guo, Boyuan Luo, and Jingren Zhou </div> <div class="periodical"> <em>In Proceedings of the USENIX Annual Technical Conference (USENIX ATC, CCF-A)</em> , Virtual Event, Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.usenix.org/system/files/atc21-zhou.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=TG3BO8muC4w&amp;feature=emb_imp_woyt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/kimihe/Octo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.usenix.org/system/files/atc21_slides_zhou.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </div> <div class="abstract hidden"> <p>On-device learning is an emerging technique to pave the last mile of enabling edge intelligence, which eliminates the limitations of conventional in-cloud computing where dozens of computational capacities and memories are needed. A high-performance on-device learning system requires breaking the constraints of limited resources and alleviating computational overhead. In this paper, we show that employing the 8-bit fixed-point (INT8) quantization in both forward and backward passes over a deep model is a promising way to enable tiny on-device learning in practice. The key to an efficient quantization-aware training method is to exploit the hardware-level enabled acceleration while preserving the training quality in each layer. However, off-the-shelf quantization methods cannot handle the on-device learning paradigm of fixed-point processing. To overcome these challenges, we propose a novel INT8 training method, which optimizes the computation of forward and backward passes via the delicately designed Loss-aware Compensation (LAC) and Parameterized Range Clipping (PRC), respectively. Specifically, we build a new network component, the compensation layer, to automatically counteract the quantization error of tensor arithmetic. We implement our method in Octo, a lightweight cross-platform system for tiny on-device learning. Evaluation on commercial AI chips shows that Octo holds higher training efficiency over state-of-the-art quantization training methods, while achieving adequate processing speedup and memory reduction over the full-precision training.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/usenix/Zhou0QGXZGLZ21</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Guo, Song and Qu, Zhihao and Guo, Jingcai and Xu, Zhenda and Zhang, Jiewei and Guo, Tao and Luo, Boyuan and Zhou, Jingren}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Calciu, Irina and Kuenning, Geoff}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Octo: {INT8} Training with Loss-aware Compensation and Backward Quantization
                    for Tiny On-device Learning}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the {USENIX} Annual Technical Conference (USENIX ATC, CCF-A)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{177--191}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{USENIX} Association}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Virtual Event}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.usenix.org/conference/atc21/presentation/zhou-qihua}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Thu, 12 Aug 2021 18:08:26 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/usenix/Zhou0QGXZGLZ21.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE TPDS’21</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/petrel_tpds21-480.webp 480w,/assets/img/publication_preview/petrel_tpds21-800.webp 800w,/assets/img/publication_preview/petrel_tpds21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/petrel_tpds21" class="preview z-depth-1 rounded" width="100%" height="auto" alt="petrel_tpds21" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:journals/tpds/ZhouGQLLGW21" class="col-sm-8"> <div class="title">Petrel: Heterogeneity-Aware Distributed Deep Learning Via Hybrid Synchronization</div> <div class="author"> <em>Qihua Zhou</em>, Song Guo, Zhihao Qu, Peng Li, Li Li, Minyi Guo, and Kun Wang </div> <div class="periodical"> <em>IEEE Transactions on Parallel and Distributed Systems (TPDS, CCF-A, IF=5.3)</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9271915" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The parameter server (PS) paradigm has achieved great success in deploying large-scale distributed Deep Learning (DL) systems. However, these systems implicitly assume that the cluster is homogeneous and this assumption does not hold in many realworld cases. Although the previous efforts are paid to address heterogeneity, they mainly prioritize the contribution of fast workers and reduce the involvement of slow workers, resulting in the limitations of workload imbalance and computation inefficiency. We reveal that grouping workers into communities, an abstraction proposed by us, and handling parameter synchronization at the community level can conquer these limitations and accelerate the training convergence progress. The inspiration of community comes from our exploration of prior knowledge about the similarity between workers, which is often neglected by previous work. These observations motivate us to propose a new synchronization mechanism named Community-aware Synchronous Parallel (CASP), which uses the Asynchronous Advantage Actor-Critic (A3C)-based algorithm to intelligently determine community configuration and fully improve the synchronization performance. The whole idea has been implemented in a prototype system called Petrel that achieves a good balance between convergence efficiency and communication overhead. The evaluation under various benchmarks with multiple metrics and baseline comparison demonstrates the effectiveness of Petrel. Specifically, Petrel accelerates the training convergence speed by up to 1.87× faster and reduces communication traffic by up to 26.85 percent, on average, over the non-community synchronization mechanisms.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/tpds/ZhouGQLLGW21</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Guo, Song and Qu, Zhihao and Li, Peng and Li, Li and Guo, Minyi and Wang, Kun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Petrel: Heterogeneity-Aware Distributed Deep Learning Via Hybrid Synchronization}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{IEEE} Transactions on Parallel and Distributed Systems (TPDS, CCF-A, IF=5.3)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1030--1043}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/TPDS.2020.3040601}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TPDS.2020.3040601}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Tue, 20 Apr 2021 11:17:38 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/journals/tpds/ZhouGQLLGW21.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE TPDS’21</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/canary_tpds21-480.webp 480w,/assets/img/publication_preview/canary_tpds21-800.webp 800w,/assets/img/publication_preview/canary_tpds21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/canary_tpds21" class="preview z-depth-1 rounded" width="100%" height="auto" alt="canary_tpds21" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:journals/tpds/ZhouWLXSG21" class="col-sm-8"> <div class="title">Canary: Decentralized Distributed Deep Learning Via Gradient Sketch and Partition in Multi-Interface Networks</div> <div class="author"> <em>Qihua Zhou</em>, Kun Wang, Haodong Lu, Wenyao Xu, Yanfei Sun, and Song Guo </div> <div class="periodical"> <em>IEEE Transactions on Parallel and Distributed Systems (TPDS, CCF-A, IF=5.3)</em>, Apr 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9252115" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The multi-interface networks are efficient infrastructures to deploy distributed Deep Learning (DL) tasks as the model gradients generated by each worker can be exchanged to others via different links in parallel. Although this decentralized parameter synchronization mechanism can reduce the time of gradient exchange, building a high-performance distributed DL architecture still requires the balance of communication efficiency and computational utilization, i.e., addressing the issues of traffic burst, data consistency, and programming convenience. To achieve this goal, we intend to asynchronously exchange gradient pieces without the central control in multi-interface networks. We propose the Piece-level Gradient Exchange and Multi-interface Collective Communication to handle parameter synchronization and traffic transmission, respectively. Specifically, we design the gradient sketch approach based on 8-bit uniform quantization to compress gradient tensors and introduce the colayerabstraction to better handle gradient partition, exchange and pipelining. Also, we provide general programming interfaces to capture the synchronization semantics and build the Gradient Exchange Index (GEI) data structures to make our approach online applicable. We implement our algorithms into a prototype system called Canary by using PyTorch-1.4.0. Experiments conducted in Alibaba Cloud demonstrate that Canary reduces 56.28 percent traffic on average and completes the training by up to 1.61x, 2.28×, and 2.84× faster than BML, Ako on PyTorch, and PS on TensorFlow, respectively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/tpds/ZhouWLXSG21</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Wang, Kun and Lu, Haodong and Xu, Wenyao and Sun, Yanfei and Guo, Song}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Canary: Decentralized Distributed Deep Learning Via Gradient Sketch
                    and Partition in Multi-Interface Networks}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{IEEE} Transactions on Parallel and Distributed Systems (TPDS, CCF-A, IF=5.3)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{900--917}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/TPDS.2020.3036738}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TPDS.2020.3036738}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Mon, 15 May 2023 18:00:34 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/journals/tpds/ZhouWLXSG21.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE TC’21</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/falcon_tc_21-480.webp 480w,/assets/img/publication_preview/falcon_tc_21-800.webp 800w,/assets/img/publication_preview/falcon_tc_21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/falcon_tc_21" class="preview z-depth-1 rounded" width="100%" height="auto" alt="falcon_tc_21" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:journals/tc/ZhouGLLGSW21" class="col-sm-8"> <div class="title">Falcon: Addressing Stragglers in Heterogeneous Parameter Server Via Multiple Parallelism</div> <div class="author"> <em>Qihua Zhou</em>, Song Guo, Haodong Lu, Li Li, Minyi Guo, Yanfei Sun, and Kun Wang </div> <div class="periodical"> <em>IEEE Transactions on Computers (TC, CCF-A, IF=3.7)</em>, Jan 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9000921" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/kimihe/Falcon" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The parameter server architecture has shown promising performance advantages when handling deep learning (DL) applications. One crucial issue in this regard is the presence of stragglers, which significantly retards DL training progress. Previous solutions for solving stragglers may not fully exploit the computation resource of the cluster as evidenced by our experiments, especially in the heterogeneous environment. This motivates us to design a heterogeneity-aware parameter server paradigm that addresses stragglers and accelerates DL training from the perspective of computation parallelism. We introduce a novel methodology named straggler projection to give a comprehensive inspection of stragglers and reveal practical guidelines to solve this problem in two aspects: (1) controlling each worker’s training speed via elastic training parallelism control and (2) transferring blocked tasks from stragglers to pioneers to fully utilize the computation resource. Following these guidelines, we propose the abstraction of parallelism as an infrastructure and design the Elastic-Parallelism Synchronous Parallel (EPSP) algorithm to handle distributed training and parameter synchronization, supporting both enforcedand slack-synchronization schemes. The whole idea has been implemented into a prototype called Falcon which effectively accelerates the DL training speed with the presence of stragglers. Evaluation under various benchmarks with baseline comparison demonstrates the superiority of our system. Specifically, Falcon reduces the training convergence time, by up to 61.83, 55.19, 38.92, and 23.68 percent shorter than FlexRR, Sync-opt, ConSGD, and DynSGD, respectively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/tc/ZhouGLLGSW21</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Guo, Song and Lu, Haodong and Li, Li and Guo, Minyi and Sun, Yanfei and Wang, Kun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Falcon: Addressing Stragglers in Heterogeneous Parameter Server Via
                    Multiple Parallelism}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{IEEE} Transactions on Computers (TC, CCF-A, IF=3.7)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{70}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{139--155}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/TC.2020.2974461}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TC.2020.2974461}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Mon, 15 May 2023 18:00:34 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/journals/tc/ZhouGLLGSW21.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE MPCE’20</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gwp_mpce20-480.webp 480w,/assets/img/publication_preview/gwp_mpce20-800.webp 800w,/assets/img/publication_preview/gwp_mpce20-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/gwp_mpce20" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gwp_mpce20" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="9290332" class="col-sm-8"> <div class="title">Learning-based Green Workload Placement for Energy Internet in Smart Cities</div> <div class="author"> <em>Qihua Zhou</em>, Yanfei Sun, Haodong Lu, and Kun Wang </div> <div class="periodical"> <em>IEEE Journal of Modern Power Systems and Clean Energy (MPCE, JCR-Q1, IF=6.3)</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9290332" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The Energy Internet is a fundamental infrastructure for deploying green city applications, where energy saving and job acceleration are two critical issues to address. In contrast to existing approaches that focus on static metrics with the assumption of complete prior knowledge of resource information, both application-level properties and energy-level requirements are realized in this paper by jointly considering energy saving and job acceleration during job runtime. Considering the online environment of smart city applications, the main objective is transferred as an optimization problem with a model partition and function assignment. To minimize the energy cost and job completion time together, a green workload placement approach is proposed by using the multi-action deep reinforcement learning method. Evaluations with real-world applications demonstrate the superiority of this method over state-of-the-art methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9290332</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Sun, Yanfei and Lu, Haodong and Wang, Kun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Journal of Modern Power Systems and Clean Energy (MPCE, JCR-Q1, IF=6.3)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning-based Green Workload Placement for Energy Internet in Smart Cities}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{91-99}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Green products;Computational modeling;Training;Neurons;Smart cities;Optimization;Runtime;Energy saving;workload scheduling;Energy Internet;green city}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.35833/MPCE.2020.000271}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE ICDCS’20</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/petrel_icdcs20-480.webp 480w,/assets/img/publication_preview/petrel_icdcs20-800.webp 800w,/assets/img/publication_preview/petrel_icdcs20-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/petrel_icdcs20" class="preview z-depth-1 rounded" width="100%" height="auto" alt="petrel_icdcs20" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:conf/icdcs/Zhou00S0G020" class="col-sm-8"> <div class="title">Petrel: Community-aware Synchronous Parallel for Heterogeneous Parameter Server</div> <div class="author"> <em>Qihua Zhou</em>, Song Guo, Peng Li, Yanfei Sun, Li Li, Minyi Guo, and Kun Wang </div> <div class="periodical"> <em>In Proceedings of the 40th IEEE International Conference on Distributed Computing Systems (ICDCS, CCF-B)</em> , Singapore, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9355829" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>As to address the impact of heterogeneity in distributed Deep Learning (DL) systems, most previous approaches focus on prioritizing the contribution of fast workers and reducing the involvement of slow workers, incurring the limitations of workload imbalance and computation inefficiency. We reveal that grouping workers into communities, an abstraction proposed by us, and handling parameter synchronization in community level can conquer these limitations and accelerate the training convergence progress. The inspiration of community comes from our exploration of prior knowledge about the similarity between workers, which is often neglected by previous work. These observations motivate us to propose a new synchronization mechanism named Community-aware Synchronous Parallel (CSP), which uses the Asynchronous Advantage Actor-Critic (A3C), a Reinforcement Learning (RL) based algorithm, to intelligently determine community configuration and fully improve the synchronization performance. The whole idea has been implemented in a system called Petrel that achieves a good balance between convergence efficiency and communication overhead. The evaluation under different benchmarks demonstrates our approach can effectively accelerate the training convergence speed and reduce synchro-nization traffic.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/icdcs/Zhou00S0G020</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Guo, Song and Li, Peng and Sun, Yanfei and Li, Li and Guo, Minyi and Wang, Kun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Petrel: Community-aware Synchronous Parallel for Heterogeneous Parameter
                    Server}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 40th {IEEE} International Conference on Distributed Computing Systems (ICDCS, CCF-B)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1183--1184}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{IEEE}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/ICDCS47774.2020.00132}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICDCS47774.2020.00132}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Tue, 02 Mar 2021 14:34:10 +0100}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/icdcs/Zhou00S0G020.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACM MM’20</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sr_mm20-480.webp 480w,/assets/img/publication_preview/sr_mm20-800.webp 800w,/assets/img/publication_preview/sr_mm20-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/sr_mm20" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sr_mm20" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:conf/mm/GuoMZZ020" class="col-sm-8"> <div class="title">Dual-view Attention Networks for Single Image Super-Resolution</div> <div class="author"> Jingcai Guo, Shiheng Ma, Jie Zhang, <em>Qihua Zhou</em>, and Song Guo </div> <div class="periodical"> <em>In Proceedings of the 28th ACM International Conference on Multimedia (ACM MM, CCF-A)</em> , Seattle, USA, Oct 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/10.1145/3394171.3413613" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>One non-negligible flaw of the convolutional neural networks (CNNs) based single image super-resolution (SISR) models is that most of them are not able to restore high-resolution (HR) images containing sufficient high-frequency information. Worse still, as the depth of CNNs increases, the training easily suffers from the vanishing gradients. These problems hinder the effectiveness of CNNs in SISR. In this paper, we propose the Dual-view Attention Networks to alleviate these problems for SISR. Specifically, we propose the local aware (LA) and global aware (GA) attentions to deal with LR features in unequal manners, which can highlight the high-frequency components and discriminate each feature from LR images in the local and global views, respectively. Furthermore, the local attentive residual-dense (LARD) block that combines the LA attention with multiple residual and dense connections is proposed to fit a deeper yet easy to train architecture. The experimental results verified the effectiveness of our model compared with other state-of-the-art methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/mm/GuoMZZ020</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo, Jingcai and Ma, Shiheng and Zhang, Jie and Zhou, Qihua and Guo, Song}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Chen, Chang Wen and Cucchiara, Rita and Hua, Xian{-}Sheng and Qi, Guo{-}Jun and Ricci, Elisa and Zhang, Zhengyou and Zimmermann, Roger}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dual-view Attention Networks for Single Image Super-Resolution}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 28th {ACM} International Conference on Multimedia (ACM MM, CCF-A)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2728--2736}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{ACM}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Seattle, USA}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3394171.3413613}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3394171.3413613}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Thu, 04 Aug 2022 14:18:09 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/mm/GuoMZZ020.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE TC’19</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/swallow_tc19-480.webp 480w,/assets/img/publication_preview/swallow_tc19-800.webp 800w,/assets/img/publication_preview/swallow_tc19-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/swallow_tc19" class="preview z-depth-1 rounded" width="100%" height="auto" alt="swallow_tc19" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:journals/tc/ZhouWLZGYG19" class="col-sm-8"> <div class="title">Fast Coflow Scheduling via Traffic Compression and Stage Pipelining in Datacenter Networks</div> <div class="author"> <em>Qihua Zhou</em>, Kun Wang, Peng Li, Deze Zeng, Song Guo, Baoliu Ye, and Minyi Guo </div> <div class="periodical"> <em>IEEE Transactions on Computers (TC, CCF-A, IF=3.7)</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8781824" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/kimihe/Swallow" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Big data analytics in datacenters often involve scheduling of data-parallel jobs. Traditional scheduling techniques based on improving network resource utilization are subject to limited bandwidth in datacenter networks. To alleviate the shortage of bandwidth, some cluster frameworks employ techniques of traffic compression to reduce transmission consumption. However, they tackle scheduling in a coarse-grained manner at task level and do not perform well in terms of flow-level metrics due to high complexity. Fortunately, the abstraction of coflow pioneers a new perspective to facilitate scheduling efficiency. In this paper, we introduce a coflow compression mechanism to minimize the completion time in data-intensive applications. Due to the NP-hardness, we propose a heuristic algorithm called Fastest-Volume-Disposal-First (FVDF) to solve this problem. For online applicability, FVDF supports stage pipelining to accelerate scheduling and exploits recurrent neural networks (RNNs) to predict compression speed. Meanwhile, we build Swallow, an efficient scheduling system that implements our proposed algorithms. It minimizes coflow completion time (CCT) while guaranteeing resource conservation and starvation freedom. The results of both trace-driven simulations and real experiments show the superiority of our algorithm, over existing one. Specifically, Swallow speeds up CCT and job completion time (JCT) by up to 1.47× and 1.66× on average, respectively, over the SEBF in Varys, one of the most efficient coflow scheduling algorithms so far. Moreover, with coflow compression, Swallow reduces data traffic by up to 48.41 percent on average.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/tc/ZhouWLZGYG19</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Wang, Kun and Li, Peng and Zeng, Deze and Guo, Song and Ye, Baoliu and Guo, Minyi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fast Coflow Scheduling via Traffic Compression and Stage Pipelining
                    in Datacenter Networks}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{IEEE} Transactions on Computers (TC, CCF-A, IF=3.7)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{68}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1755--1771}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/TC.2019.2931716}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TC.2019.2931716}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Tue, 16 Aug 2022 23:06:44 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/journals/tc/ZhouWLZGYG19.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE ICDCS’19</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/falcon_icdcs19-480.webp 480w,/assets/img/publication_preview/falcon_icdcs19-800.webp 800w,/assets/img/publication_preview/falcon_icdcs19-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/falcon_icdcs19" class="preview z-depth-1 rounded" width="100%" height="auto" alt="falcon_icdcs19" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:conf/icdcs/Zhou00L0GS19" class="col-sm-8"> <div class="title">Falcon: Towards Computation-Parallel Deep Learning in Heterogeneous Parameter Server</div> <div class="author"> <em>Qihua Zhou</em>, Kun Wang, Song Guo, Haodong Lu, Li Li, Minyi Guo, and Yanfei Sun </div> <div class="periodical"> <em>In Proceedings of the 39th IEEE International Conference on Distributed Computing Systems (ICDCS, CCF-B)</em> , Dallas, USA, Jul 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8885211" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/kimihe/Falcon" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Parameter server paradigm has shown great performance superiority for handling deep learning (DL) applications. One crucial issue in this regard is the presence of stragglers, which significantly retards DL training progress. Previous solutions for solving straggler may not fully exploit the computation capacity of a cluster as evidenced by our experiments. This motivates us to make an attempt at building a new parameter server architecture that mitigates and addresses stragglers in heterogeneous DL from the perspective of computation parallelism. We introduce a novel methodology named straggler projection to give a comprehensive inspection of stragglers and reveal practical guidelines for resolving this problem: (1) reducing straggler emergence frequency via elastic parallelism control and (2) transferring blocked tasks to pioneer workers for fully exploiting cluster computation capacity. Following the guidelines, we propose the abstraction of parallelism as an infrastructure and elaborate the Elastic-Parallelism Synchronous Parallel (EPSP) that supports both enforced-and slack-synchronization schemes. The whole idea has been implemented in a prototype called Falcon which efficiently accelerates the DL training progress with the presence of stragglers. Evaluation under various benchmarks with baseline comparison evidences the superiority of our system. Specifically, Falcon yields shorter convergence time, by up to 61.83%, 55.19%, 38.92% and 23.68% reduction over FlexRR, Sync-opt, ConSGD and DynSGD, respectively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/icdcs/Zhou00L0GS19</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Wang, Kun and Guo, Song and Lu, Haodong and Li, Li and Guo, Minyi and Sun, Yanfei}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Falcon: Towards Computation-Parallel Deep Learning in Heterogeneous
                    Parameter Server}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 39th {IEEE} International Conference on Distributed Computing Systems (ICDCS, CCF-B)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{196--206}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{IEEE}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Dallas, USA}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/ICDCS.2019.00028}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICDCS.2019.00028}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Mon, 15 May 2023 18:00:34 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/icdcs/Zhou00L0GS19.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE COMST’18</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/survey_comst-480.webp 480w,/assets/img/publication_preview/survey_comst-800.webp 800w,/assets/img/publication_preview/survey_comst-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/survey_comst" class="preview z-depth-1 rounded" width="100%" height="auto" alt="survey_comst" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:journals/comsur/WangZGL18" class="col-sm-8"> <div class="title">Cluster Frameworks for Efficient Scheduling and Resource Allocation in Data Center Networks: A Survey</div> <div class="author"> Kun Wang, <em>Qihua Zhou</em>, Song Guo, and Jiangtao Luo </div> <div class="periodical"> <em>IEEE Communications Surveys &amp; Tutorials (COMST, JCR-Q1, IF=35.6)</em>, Jul 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8416689" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Data centers are widely used for big data analytics, which often involve data-parallel jobs, including query and web service. Meanwhile, cluster frameworks are rapidly developed for data-intensive applications in data center networks (DCNs). To promote the performance of these frameworks, many efforts have been paid to improve scheduling strategies and resource allocation algorithms. With the deployment of geo-distributed data centers and data-intensive applications, the optimization in DCNs regains pervasive attention in both industry and academia. Many solutions, such as the coflow-aware scheduling and speculative execution, have been proposed to meet various requirements. Therefore, we present a solid starting ground and comprehensive overview in this area to help readers quickly understand stateof-the-art technologies and research progress. We observe that algorithms in cluster frameworks are implemented with different guidelines and can be classified according to scheduling granularity, controller management, and prior-knowledge requirement. In addition, mechanisms for conquering crucial challenges in DCNs are discussed, including providing low latency and minimizing job completion time. Moreover, we analyze desirable properties of fault tolerance and scalability to illuminate the design principles of distributed systems. We hope that this paper will shed light on this promising land and serve as a guide for further researches.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/comsur/WangZGL18</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Kun and Zhou, Qihua and Guo, Song and Luo, Jiangtao}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cluster Frameworks for Efficient Scheduling and Resource Allocation
                    in Data Center Networks: {A} Survey}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{IEEE} Communications Surveys &amp; Tutorials (COMST, JCR-Q1, IF=35.6)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{20}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3560--3580}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/COMST.2018.2857922}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/COMST.2018.2857922}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Thu, 09 Apr 2020 17:12:26 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/journals/comsur/WangZGL18.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE IPDPS’18</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/swallow_ipdps18-480.webp 480w,/assets/img/publication_preview/swallow_ipdps18-800.webp 800w,/assets/img/publication_preview/swallow_ipdps18-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/swallow_ipdps18" class="preview z-depth-1 rounded" width="100%" height="auto" alt="swallow_ipdps18" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DBLP:conf/ipps/ZhouLWZ0G18" class="col-sm-8"> <div class="title">Swallow: Joint Online Scheduling and Coflow Compression in Datacenter Networks</div> <div class="author"> <em>Qihua Zhou</em>, Peng Li, Kun Wang, Deze Zeng, Song Guo, and Minyi Guo </div> <div class="periodical"> <em>In Proceedings of the IEEE International Parallel and Distributed Processing Symposium (IPDPS, CCF-B)</em> , Vancouver, Canada, May 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8425204" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/kimihe/Swallow" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Big data analytics in datacenters often involves scheduling of data-parallel job, which are bottlenecked by limited bandwidth of datacenter networks. To alleviate the shortage of bandwidth, some existing work has proposed traffic compression to reduce the amount of data transmitted over the network. However, their proposed traffic compression works in a coarse-grained manner at job level, leaving a large optimization space unexplored for further performance improvement. In this paper, we propose a flow-level traffic compression and scheduling system, called Swallow, to accelerate data-intensive applications. Specifically, we target on coflows, which is an elegant abstraction of parallel flows generated by big data jobs. With the objective of minimizing coflow completion time (CCT), we propose a heuristic algorithm called Fastest-Volume-Disposal-First (FVDV) and implement Swallow based on Spark. The results of both trace-driven simulations and real experiments show the superiority of our system, over existing algorithms. Swallow can reduce CCT and job completion time (JCT) by up to 1.47× and 1.66× on average, respectively, over the SEBF in Varys, one of the most efficient coflow scheduling algorithms so far. Moreover, with coflow compression, Swallow reduces data traffic by up to 48.41% on average.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/ipps/ZhouLWZ0G18</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Qihua and Li, Peng and Wang, Kun and Zeng, Deze and Guo, Song and Guo, Minyi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Swallow: Joint Online Scheduling and Coflow Compression in Datacenter
                    Networks}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the {IEEE} International Parallel and Distributed Processing Symposium (IPDPS, CCF-B)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{505--514}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{IEEE} Computer Society}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Vancouver, Canada}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/IPDPS.2018.00060}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IPDPS.2018.00060}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Fri, 24 Mar 2023 00:02:03 +0100}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/ipps/ZhouLWZ0G18.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Qihua Zhou (周祺華) . </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>